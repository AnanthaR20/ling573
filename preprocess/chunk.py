"""
Script for fixed or semantic (se3-based) chunking of the billsum training data
#TODO - written 5.18.2025
I'm working on getting this script to accept a file as an input rather than a string
The code from this is being pieced together from preprocessing/simplify.py and se3/se3/segmentation.py
"""
import argparse
import pandas as pd
import re
from transformers import T5Tokenizer, T5ForConditionalGeneration
from se3.se3 import segmentation # Uncomment when se3 is in the repo

# regex = re.compile(r'(?<=\.)\s+')

# TODO: write this function to accept a file rather than a body of text
# def fixed_chunks(text:str,MAX_LEN:int) -> list[str]:
def fixed_chunks(filename:str,MAX_LEN:int) -> list[str]:
    single_column_df = pd.read_csv(filename)
    list(single_column_df[0])

    sentences = []
    current = ""
    raw_parts = re.split(r'(?<=\.)\s+', text)
    # raw_parts = regex.split(text)
    for part in raw_parts:
        part = part.strip()
        if not part:
            continue
        if len(current) + len(part) + 1 <= MAX_LEN:
            current += " " + part if current else part
        else:
            if current:
                sentences.append(current.strip())
            if len(part) <= MAX_LEN:
                current = part
            else:
                while len(part) > MAX_LEN:
                    cutoff = part.rfind(" ", 0, MAX_LEN)
                    cutoff = cutoff if cutoff != -1 else MAX_LEN
                    sentences.append(part[:cutoff].strip())
                    part = part[cutoff:].strip()
                current = part
    if current:
        sentences.append(current.strip())
    return sentences


def se3_chunks(text):
    #TODO: write this function using se3.se3.segmentation methods
    pass


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_data",help="File generated by 'sample.py' containing texts to generate a summary for")
    parser.add_argument("--type",default="fixed",help="Method by which we chunk the input text. Options are: fixed or se3")
    parser.add_argument("--fixed_chunk_size",default=200,help="parameter determining the size of the fixed chunks (only used when --type='fixed')")
    parser.add_argument("--output_file",default=None,help="parameter determining the size of the fixed chunks (only used when --type='fixed')")
    args = parser.parse_args()

    # checkpoint = "unikei/t5-base-split-and-rephrase"
    # tokenizer = T5Tokenizer.from_pretrained(checkpoint)
    # model = T5ForConditionalGeneration.from_pretrained(checkpoint)
    
    filepath = f"data/text_as_{parser.type}_chunks" if parser.output_file else parser.output_file
    with open(filepath,'w') as file:
        if parser.type == 'fixed':
            fixed = pd.DataFrame({
                "chunks": fixed_chunks(parser.input_data)
            })
            fixed.to_csv(filepath)
        elif parser.type == 'se3':
            #TODO: implement se3 chunking function
            ...
        else:
            ...

if __name__ == "__main__":
    main()






